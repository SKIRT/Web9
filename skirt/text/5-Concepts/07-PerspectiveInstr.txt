/**

\page PerspectiveInstr The perspective instrument

\section PerIntro Introduction

The PerspectiveInstrument class in SKIRT implements a full perspective view of the simulated model, with arbitrary
placement of the viewport outside or inside the model. Only photon packets arriving from the front are recorded; light
emitted behind the viewport is ignored. For each wavelength the instrument outputs the detected surface brightness in
every pixel of a given frame as a data cube in a FITS file. The instrument does \em not record an integrated flux
density.

The perspective instrument is intended mostly for making movies. Each movie frame is generated by a separate
perspective instrument with the appropriate parameters.


\section PerParam Parameters and coordinate systems

\image html PerspectiveInstrument1.png

The following table lists all parameters for the perspective instrument
other than the options common to all instrument, which obviously have no effect on the
perspective transformation. Positions and sizes are specified in the
right-handed <em>world coordinate system</em>, i.e.\ the coordinate
system in which the SKIRT model is defined, using units of length
such as parsec. Usually the model is centered around the origin \f$\mathbf{O}\f$
of the world coordinate system.

<TABLE>
<TR><TD><B>Parameter(s)</B></TD>     <TD><B>Description</B></TD></TR>
<TR><TD>\f$N_{x},N_{y}\f$</TD>       <TD>The number of pixels in the viewport in the local
                                         \f$x\f$ and \f$y\f$ directions</TD></TR>
<TR><TD>\f$S_{x},S_{y}\f$</TD>       <TD>The size (twice the extent) of the viewport in the local \f$x\f$ and
                                         \f$y\f$ directions</TD></TR>
<TR><TD>\f$V_{x},V_{y},V_{z}\f$</TD> <TD>The position of the viewport origin \f$\mathbf{V}\f$</TD></TR>
<TR><TD>\f$C_{x},C_{y},C_{z}\f$</TD> <TD>The crosshair position \f$\mathbf{C}\f$ that determines the orientation
                                         of the viewport's \f$z\f$ axis</TD></TR>
<TR><TD>\f$U_{x},U_{y},U_{z}\f$</TD> <TD>The upwards position \f$\mathbf{U}\f$ that determines the orientation
                                         of the viewport's \f$y\f$ axis</TD></TR>
<TR><TD>\f$F_{e}\f$</TD>             <TD>The focal length that determines the position of the eye</TD></TR>
</TABLE>

The aspect ratio of the viewport is assumed to be the same when calculated
in pixels (\f$N_{y}/N_{x}\f$) or in world size (\f$S_{y}/S_{x}\f$). If not
the perspective transformation will introduce a distortion.

The viewport is placed in the \f$xy\f$ plane of the left-handed <em>viewport
coordinate system</em> (left-handed so that depth increases along the
\f$z\f$-axis). Both the viewport and the coordinate system are centered
around the viewport origin \f$\mathbf{V}\f$. The \f$z\f$ axis is placed
along the crosshair line \f$\mathbf{\overline{VC}}\f$, i.e.\ the line
connecting the viewport origin with the crosshair position, in the
direction of \f$\mathbf{C}\f$. Consequently the viewport is perpendicular
to \f$\overline{\mathbf{VC}}\f$. The \f$y\f$ axis is placed such that it
is parallel with the upwards line \f$\overline{\mathbf{OU}}\f$, i.e.\ the
line connecting the \em world origin with the upwards position,
and in the same direction.

The eye and the origin of the left-handed <em>eye coordinate system </em>
are placed on the crosshair line \f$\overline{\mathbf{VC}}\f$ at a distance
\f$F_{e}\f$ from the viewport origin \f$\mathbf{V}\f$, at the opposite side
from \f$\mathbf{C}\f$. In other words, the eye is behind the viewport
and looks towards the crosshair position through the viewport. The
axes of the eye coordinate system have the same orientation as those
of the viewport coordinate system. The perspective transformation
occurs from the eye to the viewport system.


\section PerDeriv Derived positions and directions

We denote the \em norm of an arbitrary point or vector \f$\mathbf{A}\f$ as
\f[
\left\Vert \mathbf{A}\right\Vert =\sqrt{A_{x}^{2}+A_{y}^{2}+A_{z}^{2}}
\f]
A unit vector in the direction from the crosshair position to the
viewport origin can then be written as
\f[
\mathbf{G}=\frac{\mathbf{V}-\mathbf{C}}{\left\Vert \mathbf{V}-\mathbf{C}\right\Vert }
\f]
and the eye position \f$\mathbf{E}\f$ becomes
\f[
\mathbf{E}=\mathbf{V}+F_{e}\mathbf{G}
\f]
When a peel-off photon packet is launched towards the instrument
from position \f$\mathbf{P}\f$, a unit vector with the appropriate direction
is given by
\f[
\mathbf{D}=\frac{\mathbf{E}-\mathbf{P}}{\left\Vert \mathbf{E}-\mathbf{P}\right\Vert }
\f]


\section PerHomog Homogeneous coordinates

We use homogeneous coordinates \f$(x,y,z,w)\f$ with arbitrary scale factor
\f$w\f$. An arbitary point \f$\mathbf{P}=(P_{x},P_{y},P_{z})\f$ can be represented
in homogeneous coordinates as \f$\mathcal{P}=(P_{x},P_{y},P_{z},1)\f$.
An arbitrary direction \f$\mathbf{D}=(D_{x},D_{y},D_{z})\f$ is represented
as a point at infinity \f$\mathcal{D}=(D_{x},D_{y},D_{z},0)\f$.

Vice versa, given the homogeneous coordinates \f$\mathcal{Q}=(Q_{x},Q_{y},Q_{z},Q_{w})\f$,
the corresponding regular coordinates for the point can be retrieved
as \f$\mathbf{Q}=(Q_{x}/Q_{w},Q_{y}/Q_{w},Q_{z}/Q_{w})\f$ provided \f$Q_{w}\neq0\f$.

A homogeneous coordinate transformation can be written as \f$\mathcal{P}'=\mathcal{P}\,\mathcal{T}\f$
where \f$\mathcal{T}\f$ is a \f$4\times4\f$ transformation matrix which
can represent a scaling, rotation, translation, perspective transformation
or any combination thereof; or more explicitly
\f[
\left[\begin{array}{cccc}
x' & y' & z' & w'\end{array}\right]=\left[\begin{array}{cccc}
x & y & z & w\end{array}\right]\left[\begin{array}{cccc}
t_{11} & t_{12} & t_{13} & t_{14}\\
t_{21} & t_{22} & t_{23} & t_{24}\\
t_{31} & t_{32} & t_{33} & t_{34}\\
t_{41} & t_{42} & t_{43} & t_{44}
\end{array}\right]
\f]

Consecutive transformations can be combined into a single transformation
matrix through regular matrix multiplication.


\section PerTrans The instrument transformation

When a peel-off photon packet launched from position \f$\mathbf{P}\f$
arrives at the perspective instrument, a single transformation is
applied to find the homogeneous coordinates of the pixel receiving
the photon packet's luminosity
\f[
\mathcal{J}=\mathcal{P}\,\mathcal{T}_{\mathrm{tot}}
\f]
where \f$\mathcal{P}=(P_{x},P_{y},P_{z},1)\f$ and \f$\mathcal{T}_{\mathrm{tot}}\f$
represents the composite transformation from world coordinates to
pixel coordinates which will be developed below.

The incoming photon packet's luminosity is taken into account only
if the packet originated in front of the viewport and if the pixel
indices are both within range (from zero up to the number of pixels
in the relevant direction minus one). The pixel indices can be retrieved
using
\f{eqnarray*}
i & = & \mathrm{floor}(\mathcal{J}_{x}/\mathcal{J}_{w})\\
j & = & \mathrm{floor}(\mathcal{J}_{y}/\mathcal{J}_{w})
\f}
based on the standard mechanism for converting homogeneous to regular
coordinates. The perspective transformation developed below is such
that the distance \f$d\f$ from the launching position \f$\mathbf{P}\f$ to
the the viewport plane is given by
\f[
d=\left|\mathcal{J}_{z}\right|
\f]
and \f$\mathbf{P}\f$ is in front of the viewport if and only if \f$\mathcal{J}_{z}>0\f$.
In this case there is no division by \f$\mathcal{J}_{w}\f$ to avoid distortion
of the distance scale by the perspective transformation.


\subsection PerTransDev Developing the transformation

At the highest level we compose the instrument transformation as follows
\f[
\mathcal{T}_{\mathrm{tot}}=\mathcal{T}_{we}\,\mathcal{T}_{ev}\,\mathcal{T}_{vp}
\f]
where
  - \f$\mathcal{T}_{we}\f$ transforms from world coordinates to eye coordinates
  - \f$\mathcal{T}_{ev}\f$ transforms from eye coordinates to viewport coordinates
  - \f$\mathcal{T}_{vp}\f$ transforms from viewport coordinates to pixel coordinates


\subsection PerTransWtE From world to eye coordinates

The transformation from world to eye coordinates can be composed as
follows
\f[
\mathcal{T}_{we}=\mathcal{M}_{e}\,\mathcal{R}_{xy}\,\mathcal{R}_{z}\,\mathcal{F}_{z}
\f]
with from left to right:
  - translate the origin to the eye position;
  - rotate around the \f$x\f$ and \f$y\f$ axes to align the \f$z\f$ axis with the
    crosshair line \f$\overline{\mathbf{CV}}\f$, pointing in the direction
    of the eye; the \f$xy\f$ plane is now parallel with the viewport;
  - rotate around the \f$z\f$ axis to align the \f$y\f$ axis so that it is parallel
    with and in the same direction as the projection on the \f$xy\f$ plane
    of the upwards line \f$\overline{\mathbf{OU}}\f$; the \f$y\f$ axis now points
    up and the \f$x\f$ axis now points to the right when looking down from
    the positive \f$z\f$ axis;
  - flip the direction of the \f$z\f$ axis so that depth increases away from the eye.

It is easy to see that
\f[
\mathcal{M}_{e}=\left[\begin{array}{cccc}
1 & 0 & 0 & 0\\
0 & 1 & 0 & 0\\
0 & 0 & 1 & 0\\
-E_{x} & -E_{y} & -E_{z} & 1
\end{array}\right]\qquad\mathrm{and}\qquad\mathcal{F}_{z}=\left[\begin{array}{cccc}
1 & 0 & 0 & 0\\
0 & 1 & 0 & 0\\
0 & 0 & -1 & 0\\
0 & 0 & 0 & 1
\end{array}\right]
\f]

To determine \f$\mathcal{R}_{xy}\f$ let \f$(a,b,c)\f$ be the direction cosines
of the desired \f$z\f$ axis alignment, i.e.\ the direction \f$\mathbf{G}\f$
from the crosshair position to the eye position
\f[
(a,b,c)=(G_{x},G_{y},G_{z})
\f]
and define \f$v=\sqrt{b^{2}+c^{2}}\f$ as the
hypothenuse of the direction vector's projection in the \f$yz\f$ plane.
For the time being, assume that \f$v\neq0\f$.

\image html PerspectiveInstrument2.png

Alignment of the \f$z\f$ axis with \f$(a,b,c)\f$ can then be accomplished
by a rotation over \f$\alpha\f$ about the \f$x\f$ axis (figure (a)) followed
by a rotation over \f$\beta\f$ about the \f$y\f$ axis (figure (b)) -- the
angles are directed clockwise when looking down from the positive
rotation axis. The projection of vector \f$(a,b,c)\f$ in the \f$yz\f$ plane
has components \f$b\f$ along the \f$y\f$ axis and \f$c\f$ along the \f$z\f$ axis,
so that \f$\cos\alpha=c/v\f$ and \f$\sin\alpha=-b/v\f$. The situation after
the first rotation is shown in figure (b). The length of the hypothenuse
in this figure is \f$1\f$ since the direction cosines \f$(a,b,c)\f$ are
normalized. It is then immediately clear that \f$\cos\beta=v\f$ and \f$\sin\beta=-a\f$.
Thus we find
\f[
\mathcal{R}_{xy}=\left[\begin{array}{cccc}
1 & 0 & 0 & 0\\
0 & {c}/{v} & {b}/{v} & 0\\
0 & -\,{b}/{v} & {c}/{v} & 0\\
0 & 0 & 0 & 1
\end{array}\right]\left[\begin{array}{cccc}
v & 0 & a & 0\\
0 & 1 & 0 & 0\\
-a & 0 & v & 0\\
0 & 0 & 0 & 1
\end{array}\right]
\f]

To determine \f$\mathcal{R}_{z}\f$, let \f$(k,l,m)\f$ be the direction cosines
of the desired \f$y\f$ axis alignment, i.e.\ the direction from the
world origin to the upwards position, \em after partial transformation
to eye coordinates
\f[
\left[\begin{array}{cccc}
k & l & m & 1\end{array}\right]=\left[\begin{array}{cccc}
U_{x} & U_{y} & U_{z} & 1\end{array}\right]\,\mathcal{R}_{xy}
\f]
so that the projection of the upwards direction on the eye \f$xy\f$ plane
has components \f$k\f$ along the \f$x\f$ axis and \f$l\f$ along the \f$y\f$ axis.
Using the previously derived value for \f$\mathcal{R}_{xy}\f$ simple
matrix algebra leads to
\f[
k=(b^{2}+c^{2})U_{x}-abU_{y}-acU_{z}\qquad\mathrm{and}\qquad l=cU_{y}-bU_{z}
\f]
where we dropped a proportionality factor of \f$1/v\f$ in both values,
since \f$k\f$ and \f$l\f$ are not normalized.

\image html PerspectiveInstrument3.png

We need a rotation over \f$\gamma\f$ about the \f$z\f$ axis (figure (c)).
Defining \f$u=\sqrt{k^{2}+l^{2}}\f$, a reasoning
similar to the one before gives \f$\cos\gamma=l/u\f$ and \f$\sin\gamma=-k/u\f$,
and thus
\f[
\mathcal{R}_{z}=\left[\begin{array}{cccc}
{l}/{u} & {k}/{u} & 0 & 0\\
-\,{k}/{u} & {l}/{u} & 0 & 0\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & 1
\end{array}\right]
\f]

In case \f$v\approx0\f$ the matrices \f$\mathcal{R}_{xy}\f$ and \f$\mathcal{R}_{z}\f$
as derived above are not well defined and the rotations to align the
\f$z\f$ axis should be performed in reverse order, i.e. first about the
\f$y\f$ axis and then about the \f$x\f$ axis. A reasoning similar to the
above leads to the alternative
\f[
v'=\sqrt{a^{2}+c^{2}}
\f]
\f[
\mathcal{R}'_{xy}=\left[\begin{array}{cccc}
{c}/{v'} & 0 & {a}/{v'} & 0\\
0 & 1 & 0 & 0\\
-\,{a}/{v'} & 0 & {c}/{v'} & 0\\
0 & 0 & 0 & 1
\end{array}\right]\left[\begin{array}{cccc}
1 & 0 & 0 & 0\\
0 & v' & b & 0\\
0 & -b & v' & 0\\
0 & 0 & 0 & 1
\end{array}\right]
\f]
\f[
k'=cU_{x}-aU_{z}
\f]
\f[
l'=(a^{2}+c^{2})U_{y}-abU_{x}-bcU_{z}
\f]
\f[
u'=\sqrt{k'^{2}+l'^{2}}
\f]
\f[
\mathcal{R}'_{z}=\left[\begin{array}{cccc}
{l'}/{u'} & {k'}/{u'} & 0 & 0\\
-\,{k'}/{u'} & {l'}/{u'} & 0 & 0\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & 1
\end{array}\right]
\f]


\subsection PerTransEtV From eye to viewport coordinates

\image html PerspectiveInstrument4.png

The figure shows the \f$yz\f$ plane of the eye and viewport coordinate
systems. To develop the perspective transformation, note that the
triangles \f$EP'V\f$ and \f$EPQ\f$ are similar so that \f$\frac{y_{v}}{F_{e}}=\frac{y_{e}}{z_{e}}\f$.
With similar reasoning in the \f$xz\f$ plane, we get
\f{eqnarray*}
x_{v} & = & F_{e}\frac{x_{e}}{z_{e}}\\
y_{v} & = & F_{e}\frac{y_{e}}{z_{e}}
\f}
In the \f$z\f$ direction we wish to preserve the distance scale and simply
perform the translation
\f[
z_{v}=z_{e}-F_{e}
\f]

This can be accomplished through the homogeneous transformation matrix
\f[
\mathcal{T}_{ev}=\left[\begin{array}{cccc}
1 & 0 & 0 & 0\\
0 & 1 & 0 & 0\\
0 & 0 & 1 & 1/F_{e}\\
0 & 0 & -F_{e} & 0
\end{array}\right]
\f]
keeping in mind that the \f$x_{v}\f$ and \f$y_{v}\f$ coordinates will be
divided by \f$w_{v}=z_{e}/F_{e}\f$ when converting from homogeneous to
regular coordinates, and assuming that the \f$z_{v}\f$ coordinate will
not be divided by this factor. This ``trick'' for the \f$z\f$ coordinate
works only because we have set \f$w=1\f$ when converting the original
point to homogeneous coordinates, and none of the other transformation
matrices change the \f$w\f$ coordinate.


\subsection PerTransVtP From viewport to pixel coordinates

The viewport is centered on the origin and is scaled in world distances,
while pixel indices range from zero to the number of pixels in each
direction minus one. Thus we need to apply the following scaling and
translation
\f[
\mathcal{T}_{vp}=\left[\begin{array}{cccc}
{N_{x}}/{S_{x}} & 0 & 0 & 0\\
0 & {N_{y}}/{S_{y}} & 0 & 0\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & 1
\end{array}\right]\left[\begin{array}{cccc}
1 & 0 & 0 & 0\\
0 & 1 & 0 & 0\\
0 & 0 & 1 & 0\\
{N_{x}}/{2} & {N_{y}}/{2} & 0 & 1
\end{array}\right]
\f]

We leave the depth coordinate unchanged, which means it is still scaled
in world distances, zero depth corresponds with the viewport plane,
and positive depth indicates positions in front of the viewport.


\section PerFlux Flux calibration

Each photon packet arriving at the instrument carries a certain fraction of luminosity \f$L_\mathrm{pp}\f$ at a
particular wavelength \f$\lambda\f$. The contribution of the photon packet to the surface brightness
\f$f_{\lambda,_\mathrm{px}}\f$ of the target instrument pixel can be written as \f[ f_{\lambda,_\mathrm{px}} =
\frac{1}{\Delta\lambda}\frac{1}{4\pi d_\mathrm{pp}^{2}}\frac{1}{\Omega_\mathrm{px}} L_\mathrm{px} \f] where
\f$\Delta\lambda\f$ is the width of the wavelength bin containing the wavelength \f$\lambda\f$, \f$d_\mathrm{pp}\f$ is
the distance from the photon packet's origin to the instrument, and \f$\Omega_\mathrm{px}\f$ is the solid angle
subtended by the target pixel as seen from the eye position.

This solid angle depends on the location of the pixel in the viewport. Pixels at the edges or in the corners of the
viewport subtend a smaller solid angle than central pixels. The current implementation of the instrument, though, uses
the solid angle of the central pixel for all pixels in the viewport. Let us examine the nature of the error introduced
by this approximation. For simplicity, we assume a square viewport of size \f$S \times S\f$ with \f$N \times N\f$
square pixels and a distance from the viewport to the eye position (focal length) of \f$F_\mathrm{e}\f$. Further
assuming that \f$N\f$ is uneven, the solid angle \f$\Omega_0\f$ of the central viewport pixel is given by \f[ \Omega_0
= \left( 2\arctan\left(\frac{S}{2 N F_\mathrm{e}}\right) \right)^2. \f] It is straightforward to also calculate the
solid angle \f$\Omega_N\f$ of a pixel in of the corners of the viewport. The relative error \f$(\Omega_0 -
\Omega_N)/\Omega_0\f$ is plotted in the figure below (as a percentage) as a function of the ratio \f$S/F_\mathrm{e}\f$
of the viewport size and the focal length:

\image html PerspectiveSolidAngleError.png

It is thus recommended to place the eye at a distance of at least twice the size of the viewpoint. In that case, the
error in the flux calibration at the edges of the viewport is limited to about 10%. Conversely, if the focal distance
becomes smaller than the viewport size, the error quickly rises to an unacceptable level.

Lastly, to avoid overly large contributions from extremely nearby objects, the instrument ignores any photon packets
that originated at a distance to the viewport \f$d<\frac{S}{10N}\f$. (Photon packets originating behind the viewport
are ignored in any case).

*/
