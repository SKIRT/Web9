/**

\page UserMPI Using SKIRT with multiple processes

\section UserMpiHybrid Hybrid parallelization in SKIRT

\c SKIRT allows both multi-threading and multi-processing parallelization, in any combination. This is illustrated
in the following diagram:

\image html UserHybridSkirt.png

With multi-threading, the code executed by the different processors uses the same memory locations. The threads share
the entire process state, with all variables and functions. Multiple threads may attempt to read from and write to the
same memory location at the same time, which may lead to <em>race conditions</em> and unpredictable behavior. There are
mechanisms to avoid these problems, but the fact remains that, with a large number of threads, performance goes down
because all threads are competing for a common resource.

With multi-processing, the execution of parallel code is performed by multiple, independent processes, each with their
own memory addresses and process state. This avoids the performance issues from which multi-threading suffers. On the
other hand, this kind of parallelization requires the implementation of explicit calls to a Message Passing Interface
(MPI) at any point where communication is needed between processes. If implemented efficiently with a minimal amount of
communication, multi-processing can scale much better for large number of processors than multi-threading. Also,
processes can be allocated to different compute nodes, distributing the workload across a potentially large system
with a distributed memory architecture.

The combination of multi-threading and multi-processing, called \em hybrid parallelization, allows \c SKIRT to perform
efficiently on a wide range of system architectures, from laptops to supercomputers.

Photon shooting in \c SKIRT is fully parallelized. However, some of the tasks performed during the setup and output
phases of a simulation cannot be distributed across multiple processes or even across multiple threads. As a result,
speedup (compared to single-thread single-process execution) during these phases may be limited, regardless of the
number of processes and threads per process in use.

For more information on parallelization in \c SKIRT, refer to the concept note on \ref Parallelization.


\section UserMpiBuild Building SKIRT with MPI

Multi-threading is enabled in \c SKIRT by default because it does not require any external dependencies. To enable \c
SKIRT's multi-processing capabilities, however, an implementation of the Message Passing Interface (MPI) must be
installed on the host system, and the appropriate build-time option must be turned on before compiling and linking the
\c SKIRT code. For further instructions, refer to \ref InstallationGuide and specifically to \ref InstallMPI.


\section UserMpiRun Running SKIRT with MPI

Performing a \c SKIRT simulation in multi-processing mode is straightforward. You will make use of the \c mpirun
command,  which seems to be universal to all MPI implementations, although the precise syntax and semantics of the
command line arguments differs between implementations. The \c mpirun command essentially launches the requested
number of copies of the specified program, each in its own process. For example, you can launch \f$N\f$ parallel
\c SKIRT processes with the following command:

    $ mpirun -np N skirt [skirt-command-line-arguments]

The usual \c skirt command with its command line arguments is thus placed after the <tt>mpirun -np N</tt> command. For
example, a \c SKIRT simulation with 6 processes and 2 threads per process is started by entering:

    $ mpirun -np 6 skirt -t 2 galaxy.ski

As described above in \ref UserMpiHybrid, you can use any combination of the number of processes and the number of
threads. The output of the above command is essentially the same as for the regular \c skirt command. However, you will
be informed of the number of parallel processes in use as illustrated here:

    Welcome to SKIRT v__
    Constructing a simulation from ski file 'galaxy.ski'...
    Starting simulation galaxy using 2 threads for each of 6 processes...
    Starting setup...
    __

\note The \c mpirun command 'works' regardless of whether the program being launched is MPI-enabled. If you launched a
\c SKIRT simulation with the <tt>mpirun -np N</tt> command, and \c SKIRT does not log the correct number of processes,
this means that \c SKIRT was not properly built with MPI. In that case, using \c mpirun is useless, as you will have
\f$N\f$ identical instances of \c SKIRT, each performing the same simulation and trying to overwrite the same output
files.

The \c -v command line option causes each parallel process to create its own log file, rather than relying on the root
process to log all relevant information. This can be useful to analyze the progress of each individual process for
evaluating load balancing, or to debug parallelization-related issues. The \c -b command line option causes just
success and error messages to be shown on the console, rather than all progress messages. The complete log output is
still written to a file (or files) in the output directory. This option is useful when the simulation is performed as
part of a batch job, where extensive console logging would be a waste of resources. The \c -v and \c -b options can be
usefully combined; for example:

    $ mpirun -np 6 skirt -t 2 -b -v galaxy.ski


\section UserMpiWhen When to use MPI

A modern compute node (whether a laptop, desktop, server, or one of the nodes in an interconnected cluster) usually has
multiple processing cores, and can therefore host multiple processes and perform multiple execution threads in
parallel. This situation leads to two related questions:
 - When is it meaningful to run \c SKIRT with multiple processes?
 - What is the proper balance between the number of processes and the number of threads per process?

On laptops and most desktop computers, the \c SKIRT multi-threading parallization usually scales well enough that it is
not worth going through the trouble of installing MPI. And in any case, the heaviest simulations won't be performed on
these computers. On servers with 12 or more physical cores, however, splitting the load over multiple MPI processes
will most likely improve the scaling. And finally, MPI is obviously the only way to distribute the load over multiple
nodes in a larger system.

When a multi-processing job is launched, each process in the MPI group is placed on a given compute node for the
lifetime of the job, and each of these processes is assigned a given number of execution threads. The execution threads
within each process simply share the same memory address space, and the processes communicate with one another through
the MPI mechanisms, regardless of whether they reside on the same node or not.

In the \c SKIRT implementation, each process includes a copy of the entire program and of all the data needed for the
simulation. On the other hand, adding more execution threads to each process has only a negligible effect on memory
usage. As a result, launching \c SKIRT with N parallel processes requires essentialy N times as much memory as a
single-process \c SKIRT invocation, regardless of the number of parallel threads in each process.

\note A previous version of \c SKIRT offered a "data parallelization" mode in which the largest data tables were
distributed across processes as opposed to being duplicated. This mode is no longer available because the change
to variable photon packet wavelengths made its implementation impossible (at least using the same mechnism).

There are thus several conflicting considerations when determining the number of processes versus the number of threads
per process (assuming a given total number of execution threads):

 - The number of processes per node is limited by the memory requirements.
 - A large number of threads per process lowers efficiency (sometimes dramatically).
 - A large number of processes lowers efficiency as well, but much less substantially.

It is usually best to keep the number of threads per process in the range from 4 to 8, adjusting the number depending
on how many processes fit in the available memory. The optimal combination depends on the system architecture and the
precise nature of the simulation.

*/
